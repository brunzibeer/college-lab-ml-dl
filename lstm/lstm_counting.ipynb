{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_counting_sol.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9kirrQodRWG"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from os import makedirs\n",
        "from os.path import exists\n",
        "from tqdm.notebook import tqdm\n",
        "from os.path import dirname\n",
        "from random import shuffle\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SyntheticSequenceDataset(Dataset):\n",
        "\n",
        "  N = 12\n",
        "\n",
        "  def __init__(self, dataset_cache: str = 'data/synthetic_dataset.pickle', \n",
        "                force_recompute: bool = False):\n",
        "\n",
        "    self._data = None\n",
        "    self._train = True\n",
        "    self.force_recompute = force_recompute\n",
        "    self.dataset_cache   = dataset_cache\n",
        "\n",
        "    self.create_data()\n",
        "\n",
        "  def eval(self):\n",
        "    self._train = False\n",
        "  \n",
        "  def train(self):\n",
        "    self._train = True\n",
        "\n",
        "  def __len__(self):\n",
        "    return self._data[0].shape[0] if self._train else self._data[2].shape[0]\n",
        "\n",
        "  def __getitem__(self, item: int):\n",
        "    return (self._data[0][item], self._data[1][item]) if self._train else \\\n",
        "      (self._data[2][item], self._data[3][item])\n",
        "\n",
        "  def create_data(self):\n",
        "\n",
        "    if not self._data:\n",
        "      if not self.force_recompute and exists(self.dataset_cache):\n",
        "        print('Loading dataset from cache...')\n",
        "        with open(self.dataset_cache, 'rb') as dump_file:\n",
        "          dataset = pickle.load(dump_file)\n",
        "      else:\n",
        "        print('Recomputing dataset...')\n",
        "        dataset = self._compute_dataset()\n",
        "        if not exists(dirname(self.dataset_cache)):\n",
        "          makedirs(dirname(self.dataset_cache))\n",
        "        with open(self.dataset_cache, 'wb') as dump_file:\n",
        "          pickle.dump(dataset, dump_file)\n",
        "\n",
        "      # Store data\n",
        "      self._data = dataset\n",
        "\n",
        "    return self._data\n",
        "\n",
        "  def _compute_dataset(self):\n",
        "\n",
        "    num_examples    = 2 ** SyntheticSequenceDataset.N\n",
        "\n",
        "    # there are N + 1 = 0, 1, ..., N different classes\n",
        "    num_classes     = SyntheticSequenceDataset.N + 1\n",
        "\n",
        "    # How many examples to use for training (others are for test)\n",
        "    num_train_examples = int(0.8 * num_examples)\n",
        "\n",
        "    # Generate 2**N binary strings\n",
        "    data_strings = [('{0:0%db}'%self.N).format(i) for i in range(num_examples)]\n",
        "\n",
        "    # Shuffle sequences\n",
        "    shuffle(data_strings)\n",
        "\n",
        "    # Cast to numeric each generated binary string\n",
        "    data_x, data_y = [], []\n",
        "    for i in tqdm(range(num_examples)):\n",
        "      train_sequence = []\n",
        "      for binary_char in data_strings[i]:\n",
        "        value = int(binary_char)\n",
        "        train_sequence.append([value])\n",
        "      # examples are binary sequences of int {0, 1}\n",
        "      data_x.append(train_sequence)           \n",
        "        # targets are the number of ones in the sequence\n",
        "      data_y.append(np.sum(train_sequence))  \n",
        "\n",
        "    # Separate suggested training and test data\n",
        "    train_data      = np.array(data_x[:num_train_examples], dtype=np.float32)\n",
        "    train_targets   = np.array(data_y[:num_train_examples])\n",
        "    test_data       = np.array(data_x[num_train_examples:], dtype=np.float32)\n",
        "    test_targets    = np.array(data_y[num_train_examples:])\n",
        "\n",
        "    return train_data, train_targets, test_data, test_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yNtjWw9o8mM"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "synthetic_dataset = SyntheticSequenceDataset(force_recompute=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "dl = DataLoader(dataset=synthetic_dataset, batch_size=BATCH_SIZE, \n",
        "                num_workers=0, drop_last=True, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n616vdx6r5_Y"
      },
      "source": [
        "for _ in range(10):\n",
        "  (x, y) = iter(dl).next()\n",
        "  x, y = x[0], y[0]\n",
        "  print('Input: ', ''.join([str(int(xcur[0])) for xcur in x.tolist()]), f'-> y: {y.item()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ORhiW8qeL3L"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "\n",
        "class LSTM4Counting(nn.Module):\n",
        "\n",
        "  def __init__(self, num_features_in: int, hidden_dim: int, \n",
        "                num_classes: int):\n",
        "\n",
        "    super(LSTM4Counting, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.lstm = nn.LSTM(num_features_in, hidden_dim, batch_first=True)\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(hidden_dim, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_dim, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, X: torch.Tensor):\n",
        "    _, (h_n, _) = self.lstm(X)\n",
        "    h_n = h_n[0]\n",
        "    output = self.net(h_n)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxJY61OrsqOY"
      },
      "source": [
        "def eval_acc(net: nn.Module, data_loader: torch.utils.data.DataLoader, \n",
        "             device: torch.device):\n",
        "  \n",
        "  correct = 0\n",
        "  total = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for x, y in data_loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x)\n",
        "      correct += torch.sum((y == y_pred.max(1)[1])).item()\n",
        "      total += y_pred.size(0)\n",
        "  \n",
        "  return correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8bpPd_s_3zh"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-saNxwGv8Ho"
      },
      "source": [
        "from torch.optim import RMSprop\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "num_hidden      = 50\n",
        "num_epochs      = 100\n",
        "learning_rate   = 0.001\n",
        "num_features_in = 1\n",
        "num_classes     = SyntheticSequenceDataset.N + 1\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = LSTM4Counting(num_features_in=num_features_in, hidden_dim=num_hidden, \n",
        "                       num_classes=num_classes).to(device)\n",
        "\n",
        "loss_fun = nn.CrossEntropyLoss().to(device)\n",
        "opt = RMSprop(model.parameters(), learning_rate)\n",
        "\n",
        "now = datetime.now()\n",
        "train_name = f'{now.hour}:{now.minute}:{now.second}/'\n",
        "writer = SummaryWriter('./logs/' + train_name)\n",
        "  \n",
        "for e in tqdm(range(num_epochs)):\n",
        "\n",
        "  model.eval()\n",
        "  \n",
        "  synthetic_dataset.train()\n",
        "  train_acc = eval_acc(model, dl, device)\n",
        "\n",
        "  synthetic_dataset.eval()\n",
        "  test_acc = eval_acc(model, dl, device)\n",
        "\n",
        "  writer.add_scalar('Acc/train', train_acc, e)\n",
        "  writer.add_scalar('Acc/test',  test_acc, e)\n",
        "\n",
        "  model.train()\n",
        "  synthetic_dataset.train()\n",
        "\n",
        "  for i, (x, y) in enumerate(dl):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fun(y_pred, y)\n",
        "    writer.add_scalar('Loss/train', loss.cpu().item(), i + e * len(dl))\n",
        "    loss.backward()\n",
        "    opt.step()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i590f2PAeSIx"
      },
      "source": [
        "print('\\n' + 50 * '*' + '\\nInteractive Session\\n' + 50 * '*')\n",
        "\n",
        "while True:\n",
        "  my_sequence = input('Write your own binary sequence of up to %d digits in {0, 1} (write anything else to exit):\\n' % synthetic_dataset.N)\n",
        "  if len(set(my_sequence)) <= 2 and ('0' in my_sequence or '1' in my_sequence):\n",
        "\n",
        "    # Pad shorter sequences\n",
        "    if len(my_sequence) < synthetic_dataset.N:\n",
        "      my_sequence = (synthetic_dataset.N - len(my_sequence))*'0' + my_sequence\n",
        "      print(\"Sequence will be padded to\", my_sequence)\n",
        "\n",
        "    # Crop longer sequences\n",
        "    if len(my_sequence) > synthetic_dataset.N:\n",
        "      my_sequence = my_sequence[:synthetic_dataset.N]\n",
        "      print(\"Sequence will be cropped to\", my_sequence)\n",
        "\n",
        "    # Prepare example\n",
        "    test_example = []\n",
        "    for binary_char in my_sequence:\n",
        "      test_example.append([float(binary_char)])\n",
        "\n",
        "    test_example = torch.FloatTensor(test_example).unsqueeze(0).to(device)\n",
        "    y_pred = model(test_example)\n",
        "    y_pred = torch.argmax(y_pred).item()\n",
        "    y_real = int(torch.sum(test_example).item())\n",
        "    print(f'Predicted number of ones: {y_pred} - Real: {y_real}\\n')\n",
        "  else:\n",
        "    print(\"Stopping\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}