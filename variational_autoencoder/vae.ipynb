{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae_sol.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_IIrqdCMFqP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transforms = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST('/tmp/data', train=True, download=True, \n",
        "                               transform=transforms)\n",
        "\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
        "                              transform=transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHu1ajIoMr7o"
      },
      "source": [
        "BATCH_SIZE      = 64        # number of data points in each batch\n",
        "N_EPOCHS        = 10        # times to run the model on complete data\n",
        "INPUT_DIM       = 28 * 28   # size of each input\n",
        "HIDDEN_DIM      = 256       # hidden dimension\n",
        "LATENT_DIM      = 20        # latent vector dimension\n",
        "lr              = 1e-3      # learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBxovLF7MtR9"
      },
      "source": [
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EByRRl8RMvz9"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  ''' This the encoder part of VAE'''\n",
        "\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, z_dim: int):\n",
        "      '''\n",
        "      Args:\n",
        "          input_dim: A integer indicating the size of input \n",
        "            (in case of MNIST 28 * 28).\n",
        "          hidden_dim: A integer indicating the size of hidden dimension.\n",
        "          z_dim: A integer indicating the latent dimension.\n",
        "      '''\n",
        "      super(Encoder, self).__init__()\n",
        "\n",
        "      self.z_dim = z_dim\n",
        "      self.encoder = nn.Sequential(\n",
        "          nn.Linear(input_dim, hidden_dim), \n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim, hidden_dim), \n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim, 2*z_dim),\n",
        "      )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      # x is of shape [batch_size, input_dim]\n",
        "\n",
        "      hidden = self.encoder(x)\n",
        "      z_mu, z_logvar = hidden[:, :self.z_dim], hidden[:, self.z_dim:]\n",
        "\n",
        "      return z_mu, z_logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYvWzg1hM0At"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  ''' This the decoder part of VAE'''\n",
        "\n",
        "  def __init__(self, z_dim: int, hidden_dim: int, output_dim: int):\n",
        "      '''\n",
        "      Args:\n",
        "          z_dim: A integer indicating the latent size.\n",
        "          hidden_dim: A integer indicating the size of hidden dimension.\n",
        "          output_dim: A integer indicating the output dimension \n",
        "            (in case of MNIST it is 28 * 28)\n",
        "      '''\n",
        "      super(Decoder, self).__init__()\n",
        "\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(z_dim, hidden_dim), \n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim, hidden_dim), \n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim, output_dim),\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      return self.decoder(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb0e8m-MF_a_"
      },
      "source": [
        "# Variational AutoEncoder\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{\\mu}_x, \\boldsymbol{\\sigma}_x &= M(\\textbf{x}), \\Sigma(\\textbf{x}) && \\text{Push $\\textbf{x}$ through encoder}\n",
        "\\\\ \\\\\n",
        "\\boldsymbol{\\epsilon} &\\sim \\mathcal{N}(0, 1) && \\text{Sample noise}\n",
        "\\\\ \\\\\n",
        "\\textbf{z} &= \\boldsymbol{\\epsilon} \\boldsymbol{\\sigma}_x + \\boldsymbol{\\mu}_x  && \\text{Reparameterize}\n",
        "\\\\ \\\\\n",
        "\\textbf{x}_r &= p_{\\boldsymbol{\\theta}}(\\textbf{x} \\mid \\textbf{z}) && \\text{Push $\\textbf{z}$ through decoder}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrOC6YrSM85V"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    ''' This the VAE, which takes a encoder and decoder.'''\n",
        "\n",
        "    def __init__(self, enc: Encoder, dec: Decoder):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "\n",
        "    def reparameterization_trick(self, z_mu: torch.Tensor, \n",
        "                                 z_logvar: torch.Tensor):\n",
        "      \n",
        "        # sample from the distribution having latent parameters z_mu, z_logvar\n",
        "        \n",
        "        # reparameterize\n",
        "        var = torch.exp(z_logvar)\n",
        "        eps = torch.randn_like(var)\n",
        "        x_sample = eps.mul(var).add_(z_mu)\n",
        "        \n",
        "        return x_sample\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \n",
        "        # encode\n",
        "        z_mu, z_logvar = self.enc(x)\n",
        "\n",
        "        # sample z from posterior distribution\n",
        "        z_post = self.reparameterization_trick(z_mu, z_logvar)\n",
        "        \n",
        "        # decode\n",
        "        predicted = self.dec(z_post)\n",
        "\n",
        "        return predicted, z_mu, z_logvar\n",
        "\n",
        "    def sample(self, num_samples: int = 1):\n",
        "      \n",
        "      # sample z from the prior distribution\n",
        "      z = torch.randn(num_samples, LATENT_DIM).to(device)\n",
        "\n",
        "      # run only the decoder\n",
        "      rec = model.dec(z)\n",
        "\n",
        "      return rec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1s5HnYhGTWc"
      },
      "source": [
        "# ELBO computation\n",
        "\n",
        "\\begin{align}\n",
        "\\text{recon. loss} &= \\text{MSE}(\\textbf{x}, \\textbf{x}_r) \\ \\ \\text{or} \\ \\  \\text{BCE}(\\textbf{x}, \\textbf{x}_r) && \\text{Compute reconstruction loss}\n",
        "\\\\ \\\\\n",
        "\\text{var. loss} &= \\text{KL}[\\mathcal{N}(\\boldsymbol{\\mu}_x, \\boldsymbol{\\sigma}_x) \\lVert \\mathcal{N}(0, I)] && \\text{Compute variational loss}\n",
        "\\\\ \\\\\n",
        "\\text{L} &= \\text{recon. loss} + \\text{var. loss} && \\text{Combine losses}\n",
        "\\end{align}\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{aligned}\n",
        "\\text{KL}[\\mathcal{N}(\\mu_1, \\Sigma_1) \\lVert \\mathcal{N}(\\mu_2, \\Sigma_2))] &= \\\\ \n",
        "& \\frac{1}{2}\\left[\\log\\frac{|\\Sigma_2|}{|\\Sigma_1|} - d + \\text{tr} \\{ \\Sigma_2^{-1}\\Sigma_1 \\} + (\\mu_2 - \\mu_1)^T \\Sigma_2^{-1}(\\mu_2 - \\mu_1)\\right].\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI-ZQU2FQlQu"
      },
      "source": [
        "class ELBO(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ELBO, self).__init__()\n",
        "\n",
        "  def compute_rec_error_(self, x: torch.Tensor, x_rec: torch.Tensor):\n",
        "    return F.binary_cross_entropy(x_rec, x, size_average=False)\n",
        "  \n",
        "  def compute_kl_(sefl, z_mu: torch.Tensor, z_logvar: torch.Tensor):\n",
        "    return 0.5 * torch.sum(torch.exp(z_logvar) + z_mu**2 - 1.0 - z_logvar)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, x_rec: torch.Tensor, \n",
        "              z_mu: torch.Tensor, z_logvar: torch.Tensor): \n",
        "\n",
        "    # reconstruction loss\n",
        "    recon_loss = self.compute_rec_error_(x, x_rec)\n",
        "\n",
        "    # kl divergence loss\n",
        "    kl_loss = self.compute_kl_(z_mu, z_logvar)\n",
        "\n",
        "    # total loss\n",
        "    loss = recon_loss + kl_loss\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D38EzoMeNMof"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def eval(data_loader: DataLoader, model: VAE):\n",
        "    \n",
        "    # set the evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # test loss for the data\n",
        "    test_loss = 0\n",
        "    num_examples = 0\n",
        "\n",
        "    # define the ELBO loss function\n",
        "    loss = ELBO()\n",
        "\n",
        "    # we don't need to track the gradients, since we are not updating the \n",
        "    # parameters during evaluation / testing\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(data_loader):\n",
        "            # reshape the data\n",
        "            x = x.view(-1, 28 * 28)\n",
        "            x = x.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            x_rec, z_mu, z_logvar = model(x)\n",
        "\n",
        "            test_loss += loss(x, x_rec, z_mu, z_logvar).item()\n",
        "            num_examples += len(x)\n",
        "\n",
        "    return test_loss/num_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYXv6y97NCQ2"
      },
      "source": [
        "encoder   = Encoder(INPUT_DIM, HIDDEN_DIM, LATENT_DIM) # encoder\n",
        "decoder   = Decoder(LATENT_DIM, HIDDEN_DIM, INPUT_DIM) # decoder\n",
        "model     = VAE(encoder, decoder).to(device) # vae\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)  # optizer\n",
        "loss_fun  = ELBO()\n",
        "\n",
        "for e in range(N_EPOCHS):\n",
        "\n",
        "    # set the train mode\n",
        "    model.train()\n",
        "    \n",
        "    for i, (x, _) in enumerate(train_iterator):\n",
        "       \n",
        "        # reshape the data into [batch_size, 784]\n",
        "        x = x.view(-1, 28 * 28).to(device)\n",
        "\n",
        "        # update the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        x_rec, z_mu, z_logvar = model(x)\n",
        "\n",
        "        loss = loss_fun(x, x_rec, z_mu, z_logvar)\n",
        "        \n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {e}, Train Loss: {eval(train_iterator, model):.2f}, \\\n",
        "          Test Loss: {eval(test_iterator, model):.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va1ymJQJuh9L"
      },
      "source": [
        "# Example: sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtPqUgqbNUkT"
      },
      "source": [
        "img = model.sample()\n",
        "img = img.view(28, 28)\n",
        "img = img.cpu().detach().numpy()\n",
        "\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy5osv4julLH"
      },
      "source": [
        "# Example: interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9726BwmlxKd"
      },
      "source": [
        "number_one = train_dataset.data[train_dataset.targets == 1][0]\n",
        "number_five = train_dataset.data[train_dataset.targets == 5][0]\n",
        "\n",
        "with torch.no_grad():\n",
        "  one_mu, one_logvar = model.enc((number_one.view(-1, 28 * 28) / 255.).to(device))\n",
        "  z_one = model.reparameterization_trick(one_mu, one_logvar)\n",
        "  five_mu, five_logvar = model.enc((number_five.view(-1, 28 * 28) / 255.).to(device))\n",
        "  z_five = model.reparameterization_trick(five_mu, five_logvar)\n",
        "\n",
        "  fig, ax = plt.subplots(1, 7, figsize=(21,3))\n",
        "  for n, i in enumerate(torch.linspace(0, 1, 7)):\n",
        "    predicted = model.dec(z_five * i + z_one * (1-i)).cpu()\n",
        "    ax[n].imshow(predicted.view(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTsQEP-owP7J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}